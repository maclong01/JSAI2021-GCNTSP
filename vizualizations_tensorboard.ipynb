{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pprint as pp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from problems.tsp.problem_tsp import TSP\n",
    "from utils import load_model, move_to\n",
    "from train import set_decode_type\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class opts:\n",
    "    dataset_path = \"data/tsp/tsp10-200_concorde.txt\"\n",
    "    batch_size = 16\n",
    "    accumulation_steps = 80\n",
    "    num_samples = 25600 # 1280 samples per TSP size \n",
    "    \n",
    "    neighbors = 0.20\n",
    "    knn_strat = 'percentage'\n",
    "    \n",
    "    # model = \"outputs/tsp_20-50/rl-ar-var-20pnn-gnn-max-ln_20200313T125908\"\n",
    "#     model = \"outputs/tsp_20-50/rl-ar-var-20pnn-gnn-max_20200313T002243\"\n",
    "    \n",
    "#     model = \"outputs/tspsl_20-50/sl-ar-var-20pnn-gnn-sum_20200310T094801\"\n",
    "#     model = \"outputs/tspsl_20-50/sl-ar-var-20pnn-gnn-max_20200308T172931\"\n",
    "#     model = \"outputs/tspsl_20-50/sl-ar-var-20pnn-gnn-mean_20200310T094833\"\n",
    "#     model = \"outputs/tspsl_20-50/sl-ar-var-full-mlp_20200306T182155\"\n",
    "    \n",
    "#     model = \"outputs/tspsl_20-50/sl-ar-var-20pnn-gnn-max-bntrack_20200310T095509\"\n",
    "#     model = \"outputs/tspsl_20-50/sl-ar-var-20pnn-gnn-max-ln_20200310T095955\"\n",
    "\n",
    "    model = \"outputs/tsp_20-50/rl-ar-var-20pnn-gnn-max-gaggr-sum_20200411T145725\"\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model from outputs/tsp_20-50/rl-ar-var-20pnn-gnn-max-gaggr-sum_20200411T145725/epoch-99.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AttentionModel(\n",
       "  (init_embed): Linear(in_features=2, out_features=128, bias=True)\n",
       "  (embedder): GNNEncoder(\n",
       "    (init_embed_edges): Embedding(2, 128)\n",
       "    (layers): ModuleList(\n",
       "      (0): GNNLayer(\n",
       "        (U): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (V): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (A): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (B): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (C): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm_h): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (norm_e): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "      (1): GNNLayer(\n",
       "        (U): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (V): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (A): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (B): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (C): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm_h): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (norm_e): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "      (2): GNNLayer(\n",
       "        (U): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (V): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (A): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (B): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (C): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm_h): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (norm_e): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (project_node_embeddings): Linear(in_features=128, out_features=384, bias=False)\n",
       "  (project_fixed_context): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (project_step_context): Linear(in_features=256, out_features=128, bias=False)\n",
       "  (project_out): Linear(in_features=128, out_features=128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, model_args = load_model(opts.model, extra_logging=True)\n",
    "model.to(opts.device)\n",
    "set_decode_type(model, \"greedy\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_logger = SummaryWriter(os.path.join(\n",
    "    model_args[\"log_dir\"], \"{}_{}-{}\".format(model_args[\"problem\"], model_args[\"min_size\"], model_args[\"max_size\"]), model_args[\"run_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading from data/tsp/tsp10-200_concorde.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 25600/25600 [00:04<00:00, 5215.21it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = TSP.make_dataset(\n",
    "    filename=opts.dataset_path, batch_size=opts.batch_size, num_samples=opts.num_samples, \n",
    "    neighbors=opts.neighbors, knn_strat=opts.knn_strat, supervised=True\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=opts.batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1600/1600 [15:55<00:00,  1.67it/s]  \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    graph_embs = None  # Store all graph embeddings for TB-projector \n",
    "    graph_embs_meta = None\n",
    "    \n",
    "    node_embs = None  # Reset after logging\n",
    "    log_p = None\n",
    "    log_p_selected = None\n",
    "    \n",
    "    for bat_idx, bat in enumerate(tqdm(dataloader, ascii=True)):\n",
    "        x = move_to(bat['nodes'], opts.device)\n",
    "        graph = move_to(bat['graph'], opts.device)\n",
    "        cost, ll, pi = model(x, graph, return_pi=True)\n",
    "        \n",
    "        if node_embs is None:\n",
    "            node_embs = model.embeddings_batch.cpu().numpy()\n",
    "        else:\n",
    "            # Append to node embeddings\n",
    "            node_embs = np.concatenate((node_embs, model.embeddings_batch.cpu().numpy()), axis=0)\n",
    "        \n",
    "        if log_p is None:\n",
    "            log_p = model.log_p_batch.cpu().numpy()\n",
    "        else:\n",
    "            # Append to log probabilities\n",
    "            log_p = np.concatenate((log_p, model.log_p_batch.cpu().numpy()), axis=0)\n",
    "        \n",
    "        if log_p_selected is None:\n",
    "            log_p_selected = model.log_p_sel_batch.cpu().numpy()\n",
    "        else:\n",
    "            # Append to log probabilities\n",
    "            log_p_selected = np.concatenate((log_p_selected, model.log_p_sel_batch.cpu().numpy()), axis=0)\n",
    "        \n",
    "        if (bat_idx+1) % opts.accumulation_steps == 0:\n",
    "            if graph_embs is None:\n",
    "                graph_embs = node_embs.mean(1)\n",
    "                graph_embs_meta = [f\"TSP{10* ((bat_idx+1)//opts.accumulation_steps)}\"]*len(node_embs)\n",
    "            else:\n",
    "                graph_embs = np.concatenate((graph_embs, node_embs.mean(1)), axis=0)\n",
    "                graph_embs_meta += [f\"TSP{10* ((bat_idx+1)//opts.accumulation_steps)}\"]*len(node_embs)\n",
    "            \n",
    "            # Log prediction probabilities (for all action and selected actions)\n",
    "            tb_logger.add_histogram('probs', np.exp(log_p.flatten()), 10* ((bat_idx+1)//opts.accumulation_steps))\n",
    "            tb_logger.add_histogram('probs_selected', np.exp(log_p_selected.flatten()), 10* ((bat_idx+1)//opts.accumulation_steps))\n",
    "            \n",
    "            # Log histograms of raw values\n",
    "            tb_logger.add_histogram('emb_values', node_embs.flatten(), 10* ((bat_idx+1)//opts.accumulation_steps))\n",
    "            tb_logger.add_histogram('graph_emb_values', graph_embs.flatten(), 10* ((bat_idx+1)//opts.accumulation_steps))\n",
    "            \n",
    "            # Log histograms of L2 norms\n",
    "            tb_logger.add_histogram('emb_2norm', np.linalg.norm(node_embs, axis=-1).flatten(), 10* ((bat_idx+1)//opts.accumulation_steps))\n",
    "            tb_logger.add_histogram('graph_emb_2norm', np.linalg.norm(graph_embs, axis=-1).flatten(), 10* ((bat_idx+1)//opts.accumulation_steps))\n",
    "            \n",
    "            # Log histogram of distances between node embeddings (within each graph)\n",
    "            node_embs_dists = []\n",
    "            for node_emb in node_embs:\n",
    "                # compute pdist for node embeddings within each graph\n",
    "                node_embs_dists.append(pdist(node_emb, metric='euclidean'))\n",
    "            tb_logger.add_histogram('emb_dist', np.array(node_embs_dists).flatten(), 10* ((bat_idx+1)//opts.accumulation_steps))\n",
    "            \n",
    "            # Log histogram of distances between graph embeddings\n",
    "            graph_embs_dists = pdist(graph_embs, metric='euclidean')\n",
    "            tb_logger.add_histogram('graph_emb_dist', np.array(graph_embs_dists).flatten(), 10* ((bat_idx+1)//opts.accumulation_steps))\n",
    "            \n",
    "            node_embs = None\n",
    "            log_p = None\n",
    "            log_p_selected = None\n",
    "    \n",
    "    # Log graph embeddings to projector\n",
    "    tb_logger.add_embedding(graph_embs, metadata=graph_embs_meta, tag='graph_emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
